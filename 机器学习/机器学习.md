### 创建虚拟环境

```
# win10创建虚拟环境
virtualenv -p d:\softwareintsall\python\python.exe virtualenv1
# 启动虚拟环境时需要先切换到虚拟环境的Scripts目录下再执行如下命令。
activate
#退出虚拟环境
deactivate 
```

#### tf-idf含义

```python
# tf: term frequency:词的频率
# idf: inverse document frequency:逆文档频率 == log(总文档数量/该词出现的文档数量)
# tf*idf:重要性程度
```

- 数据预处理使用的模块 **sklearn.preprocessing**



### 类别性数据的处理方式（one-hot编码）

```
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
```



### 数值型数据的处理方式（归一化，标准化）

#### 归一化（一般不使用）

缺点：鲁棒性（稳定性）较差，受异常点的影响

只适合传统精确小数点场景

#### 标准化（常使用）

**注意事项：新版本在使用标准化时必须指定数据为二维数组，否则会报错，使用属性 reshape 可以自由转换**

优点：不容易受异常点的影响，利用方差来求标准差，方差考量数据的稳定性

适合现代嘈杂大数据场景

```python
from sklearn.preprocessing import MinMaxScaler  # 归一化
mms = MinMaxScaler(feature_range=(2, 3))  # 缩放特征值到给定范围
data = mms.fit_transform([[12, 23, 13, 33], [92, 63, 73, 33], [15, 93, 33, 35]])
print(data)
# 1.目的：使得某一个特征不会对最终结果造成更大的影响
# 公式： 
#	X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
# 	X_scaled = X_std * (max - min) + min

from sklearn.preprocessing import StandardScaler  # 标准化
st = StandardScaler()
data = st.fit_transform([[12, 23, 13, 33], [92, 63, 73, 33], [15, 93, 33, 35]])
print(data)

# scale效果同StandardScaler
from sklearn.preprocessing import scale
import numpy as np
def t_scale():
	"""标准化缩放"""
	x_train = np.array([[12, 23, 13, 33], [92, 63, 73, 33], [15, 93, 33, 35]])
	print(x_train)
	data = scale(x_train)
	# data = scale([[12, 23, 13, 33], [92, 63, 73, 33], [15, 93, 33, 35]])
	print(data)
```



### SimpleImputer语法

完成缺失值插补，**一般按列进行插补**

```python
from sklearn.impute import SimpleImputer
import numpy as np
# strategy填补的策略，mean是平均值填补
# verbose填补按行还是列，0按列平均值填补，1按行平均值填补
it = SimpleImputer(missing_values=np.nan, strategy='mean', verbose=0)
data = it.fit_transform([[12, 23, 13, 33], [92, np.nan, 73, 33], [15, 93, 33, 35]])
print(data)
```



### 数据降维

- **特征选择**
- 主要方法
  
  1. filter（过滤式）：VarianceThreshold（方差过滤）
    2. Embedded（嵌入式）：正则化、决策树
    3. Wrapper（包裹式）
    - 目的：
    1.   减少特征数量、降维，使模型泛化能力更强，减少过拟合；
          2. 增强对特征和特征值之间的理解

- **PCA分析**

```
from sklearn.decomposition import PCA
# 本质：PCA是一种分析、简化数据集的技术
# 目的：是数据降维压缩，尽可能降低原数据的维度（复杂度），会损失少量信息
# 作用：可以削减回归分析或者聚类分析中特征的数量
```

​	安装jupyter模板：pip install jupyter

​	在jupyter上编写python代码，启动jupyter：jupyter notebook

​	在jupyter编写代码可以看见表的信息



### 估计器

```python
# 以下是用于分类的估计器
from sklearn.neighbors import *  # k邻近算法
from sklearn.naive_bayes import *  # 贝叶斯算法
from sklearn.linear_model import LogisticRegression  # 逻辑回归
from sklearn.tree import * # 决策树和随机森林
# 以下是用于回归的估计器
from sklearn.linear_model import LogisticRegression  # 线性回归
from sklearn.linear_model import Ridge  # 岭回归
```



### 机器学习开发流程

1. #### 拿到原始数据

   1. 明确问题做什么
   2. 根据数据类型划分应用种类

2. #### 数据的基本处理

   1. pandas去处理数据（缺失值、合并表等等）

3. #### 特征工程

   1. 分类或者回归处理
   2. 制作模型（算法 + 数据）
   3. 找到合适的方法进行预测

4. #### 模型评估

   1. （合格）上线使用，以API形式提供
   2. （不合格）换算法，换参数

### k 近邻算法（实际工程中一般不用）

```python
# k 近邻算法需要做标准化处理
# k 近邻算法公式
# 根号((a1-b1)^2+(a2-b2)^2+(a3-b3)^2)
from sklearn.neighbors import KNeighborsClassifier  # k近邻算法API

# 进行算法流程  knn估计器流程
knn = KNeighborsClassifier(n_neighbors=5)  # k值设为5  ==> n_neighbors=5
knn.fit(x_train, y_train)
# 得出预测结果
y_predict = knn.predict(x_test)
print(f'预测的目标签到位置为：{y_predict}')
# 得出准确率
print(f'预测的准确率：{knn.score(x_test, y_test)}')
```

##### k 取多大的影响

- k 值取小：容易受异常点的影响
- k 值取大：容易受k值数量（类别）波动

##### 优点：

- 简单，易于理解，易于实现，无需估计参数，无需训练

##### 缺点：

- 懒惰算法，对测试样本分类时的计算量大，内存开销大

- 必须指定k值，k值选择不当则分类精度不能保证

 ##### 使用场景：

- 小数据场景，几千 几万样本，具体场景具体业务去测试

```python
API: sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='auto')
        参数：
            n_neighbors: int 可选默认为5（邻居数） ，k_neighbors 查询默认使用的邻居数
            
            algorithm：{'auto', 'ball_tree', 'kd_tree', 'brute'} 可选用与计算最近邻居的算法
            'ball_tree' 将会使用balltrue， kd_tree将使用KDtree ‘auto’ 将尝试更具传递给fit放大的值来决定最合适的算法。（不同实现方法影响效率）
            
        knn = KNeighborsClassifier()
        knn.fit(data) 传入数据
        knn.score(x_test, y_test)  
        knn.predict(x_test)  # 预测特征测试集的目标集
```



### 朴素贝叶斯算法

- 算法不需要调参
- 朴素贝叶斯算法擅长文本分类
- 神经网络对文本分类会比这个方法的效果更好

```python
# 朴素贝叶斯算法公式
# P(科技|影院，支付宝，云计算)=P(影院，支付宝，云计算|科技)*P(科技)
from sklearn.naive_bayes import MultinomialNB  # 朴素贝叶斯算法api

# 进行朴素贝叶斯算法的预测
mlt = MultinomialNB(alpha=1.0)
mlt.fit(x_train, y_train)
y_predict = mlt.predict(x_test)
print(f'预测的准确率为：{mlt.score(x_test, y_test)}')
```

- ##### 朴素贝叶斯算法优点

  - 属性可以离散可以连续
  - 数学基础扎实,分类效率稳定
  - 假设简单,但是在很多实际情况下表现很好
  - 速度较快,一定程度上缓解了维度灾难

- ##### 朴素贝叶斯算法缺点

  - 由于使用样本独立性的假设，所以如果样本属性有关联时，其效果不好
  - 对噪音数据与缺失数据不太敏感
  - 由于训练集当中进行统计词这些工作，会对结果造成干扰



### 交叉验证（通常搭配超参数一起使用）

- 将拿到的训练数据，分为训练集和验证集。以下图为例：将数据分成5份，其中一份作为验证集。然后经过5次（组）的测试，每次都更换不同的验证集。即得到5组模型的结果，取平局值作为最终结果。又称5折交叉验证。

![交叉验证](D:\FileDocument\零碎文件\Django\Python-\机器学习\交叉验证过程.png)

![交叉验证2](D:\FileDocument\零碎文件\Django\Python-\机器学习\交叉验证.png)



### 超参数

- ```python
  # 超参数API
  from sklearn.model_selection import GridSearchCV
  ```
  - **estimator**：所使用的分类器，如estimator=RandomForestClassifier(min_samples_split=100,min_samples_leaf=20,max_depth=8,max_features='sqrt',random_state=10), 并且传入除需要确定最佳的参数之外的其他参数。每一个分类器都需要一个scoring参数，或者score方法。
  - **param_grid**：值为字典或者列表，即需要最优化的参数的取值，param_grid = {'n_neighbors': [3, 5, 10]} 。
  - **scoring**：准确度评价标准，默认None, 这时需要使用score函数；或者如scoring='roc_auc'，根据所选模型不同，评价准则不同。字符串（函数名），或是可调用对象，需要其函数签名形如：scorer(estimator, X, y)；如果是None，则使用estimator的误差估计函数。
  - **cv**：交叉验证参数，默认None，使用三折交叉验证。指定fold数量，默认为3，也可以是yield训练/测试数据的生成器。
  - **refit**：默认为True,程序将会以交叉验证训练集得到的最佳参数，重新对所有可用的训练集与开发集进行，作为最终用于性能评估的最佳模型参数。即在搜索参数结束后，用最佳参数结果再次fit一遍全部数据集。
  - **iid**：默认True,为True时，默认为各个样本fold概率分布一致，误差估计为所有样本之和，而非各个fold的平均。
  - **verbose**：日志冗长度，int：冗长度，0：不输出训练过程，1：偶尔输出，>1：对每个子模型都输出。
  - **n_jobs**：并行数，int：个数，-1：跟CPU核数一致, 1:默认值。
  - **pre_dispatch**：指定总共分发的并行任务数。当n_jobs大于1时，数据将在每个运行点进行复制，这可能导致OOM，而设置pre_dispatch参数，则可以预先划分总共的job数量，使数据最多被复制pre_dispatch次



### 分类算法

- #### 决策树

  - ##### 决策树的划分依据之一（信息增益）

    - 公式：g(D, A) = H(D) - H(D|A)
    - H(D) : ![信息熵H(D)](D:\FileDocument\零碎文件\Django\Python-\机器学习\信息熵计算.png)
    - g: 信息增益
    - A: 特征
    - D: 数据集

  - ##### 决策树常用的算法

    - ID3：信息增益        最大的准则
    - C4.5：信息增益比        最大的准则
    - CART：
      1. 回归树：评价误差   最小
      2. 分类树：基尼系数   最小的准则   在sklearn中可以选择划分的默认原则

  - ##### 决策树的API

    - **sklearn.tree.DecisionTreeClassifier**(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)

    - **criterion：**选择结点划分质量的度量标准，默认使用‘gini’，即基尼系数，基尼系数是CART算法中采用的度量标准，该参数还可以设置为 “entropy”，表示信息增益，是C4.5算法中采用的度量标准。

    - **splitter：**结点划分时的策略，默认使用‘best’。‘best’ 表示依据选用的**criterion**标准**，**选用最优划分属性来划分该结点，一般用于训练样本数据量不大的场合，因为选择最优划分属性需要计算每种候选属性下划分的结果；该参数还可以设置为“random”，表示最优的随机划分属性，一般用于训练数据量较大的场合，可以减少计算量，但是具体如何实现最优随机划分暂时不太明白，这需要查看该部分的源码。

    - **max_depth：**设置决策树的最大深度，默认为None。None表示不对决策树的最大深度作约束，直到每个叶子结点上的样本均属于同一类，或者少于**min_samples_leaf**参数指定的叶子结点上的样本个数。也可以指定一个整型数值，设置树的最大深度，在样本数据量较大时，可以通过设置该参数提前结束树的生长，改善过拟合问题，但一般不建议这么做，过拟合问题还是通过剪枝来改善比较有效。

    - **min_samples_split：**当对一个内部结点划分时，要求该结点上的最小样本数，默认为2。

    - **min_samples_leaf：**设置叶子结点上的最小样本数，默认为1。当尝试划分一个结点时，只有划分后其左右分支上的样本个数不小于该参数指定的值时，才考虑将该结点划分，换句话说，当叶子结点上的样本数小于该参数指定的值时，则该叶子节点及其兄弟节点将被剪枝。在样本数据量较大时，可以考虑增大该值，提前结束树的生长。

    - **min_weight_fraction_leaf** ：在引入样本权重的情况下，设置每一个叶子节点上样本的权重和的最小值，一旦某个叶子节点上样本的权重和小于该参数指定的值，则该叶子节点会联同其兄弟节点被减去，即其父结点不进行划分。该参数默认为0，表示不考虑权重的问题，若样本中存在较多的缺失值，或样本类别分布偏差很大时，会引入样本权重，此时就要谨慎设置该参数。

    - **max_features：**划分结点、寻找最优划分属性时，设置允许搜索的最大特征数量，默认为None。假设训练集中包含的属性个数为n，None表示搜索全部n个的候选属性；‘auto’表示最多搜索sqrt(n)个属性；sqrt表示最多搜索sqrt(n)个属性；‘log2’表示最多搜索log2(n)个属性；用户也可以指定一个整数k，表示最多搜索k个属性。需要说明的是，尽管设置了参数**max_features**，但是在至少找到一个有效（即在该属性上划分后，**criterion**指定的度量标准有所提高）的划分属性之前，最优划分属性的搜索不会停止。

    - **random_state :**当将参数**splitter**设置为‘random’时，可以通过该参数设置随机种子号，默认为None，表示使用np.random产生的随机种子号。

    - **max_leaf_nodes** **:** 设置决策树的最大叶子节点个数，该参数与**max_depth**等参数参数一起，限制决策树的复杂度，默认为None，表示不加限制。

    - **min_impurity_decrease :**打算划分一个内部结点时，只有当划分后不纯度(可以用**criterion**参数指定的度量来描述)减少值不小于该参数指定的值，才会对该结点进行划分，默认值为0。可以通过设置该参数来提前结束树的生长。

    - **min_impurity_split** **:** 打算划分一个内部结点时，只有当该结点上的不纯度不小于该参数指定的值时，才会对该结点进行划分，默认值为1e-7。该参数值0.25版本之后将取消，由**min_impurity_decrease**代替**。**

    - **class_weight：**设置样本数据中每个类的权重，这里权重是针对整个类的数据设定的，默认为None，即不施加权重。用户可以用字典型或者字典列表型数据指定每个类的权重，假设样本中存在4个类别，可以按照 [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] 这样的输入形式设置4个类的权重分别为1、5、1、1，而不是 [{1:1}, {2:5}, {3:1}, {4:1}]的形式。该参数还可以设置为‘balance’，此时系统会按照输入的样本数据自动的计算每个类的权重，计算公式为：n_samples / ( n_classes * np.bincount(y) )，其中n_samples表示输入样本总数，n_classes表示输入样本中类别总数，np.bincount(y) 表示计算属于每个类的样本个数，可以看到，属于某个类的样本个数越多时，该类的权重越小。若用户单独指定了每个样本的权重，且也设置了**class_weight**参数，则系统会将该样本单独指定的权重乘以**class_weight**指定的其类的权重作为该样本最终的权重。

    - **presort** : 设置对训练数据进行预排序，以提升结点最优划分属性的搜索，默认为False。在训练集较大时，预排序会降低决策树构建的速度，不推荐使用，但训练集较小或者限制树的深度时，使用预排序能提升树的构建速度。

    - **泰坦尼克号案例：**

      ```python
      DictVectorizer(sparse=False)  # sparse=False意思是不用稀疏矩阵表示
      ```

  - ##### 决策树可视化导出

    - API为export_graphviz

      ```python
      from sklearn.tree import DecisionTreeClassifier, export_graphviz
      ```

    - 使用graphviz将dot文件转换成png或pdf格式

      ```python
      # 软件链接：https://graphviz.gitlab.io/_pages/Download/windows/graphviz-2.38.msi
      # 将路径D:\SoftwareIntsall\Graphviz\bin加入到系统环境变量Path中
      # 打开黑窗输入：dot -version 查看graphviz有没有安装成功
      # 成功后进入到dot文件夹下使用命令进行pdf转换：dot -Tpdf tree.dot -o tree.pdf
      # 或转换成png格式：dot -Tpng tree.dot -o tree.png
      ```

      

- #### 随机森林（决策树升级版）==> 

  - ##### 建立多个决策树（随机有放回的抽样）
  
    - *多颗随机树投票决定使用*
  
    1. 随机在N个样本中选择一个样本，重复N次（样本可能重复）
    2. 随机在M个特征中选出m个特征（m<M）
    3. 建立的多个随机树样本、特征都不一样
  
  - ##### 随机森林API
  
    - ```python
      from sklearn.ensemble import RandomForestClassifier  # 导入随机森林预测API
      from sklearn.model_selection import GridSearchCV  # 网格搜索
      ```
  
    - 通常配合超参数一起使用（网格搜索GridSearchCV）



### 回归算法

- #### 线性回归

  - ```python
    # 相应的API
    # LinearRegression 正规方程API：普通最小二乘线性回归
    # coef_:回归系数
    # SGDRegressor 梯度下降API
    from sklearn.linear_model import LinearRegression, SGDRegressor
    std_y.inverse_transform(y_predict)  # inverse_transform用于标准化后还原真实数据
    ```

    















