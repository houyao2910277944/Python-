### 创建虚拟环境

```
# win10创建虚拟环境
virtualenv -p d:\softwareintsall\python\python.exe virtualenv1
# 启动虚拟环境时需要先切换到虚拟环境的Scripts目录下再执行如下命令。
activate
#退出虚拟环境
deactivate 
```

#### tf-idf含义

```python
# tf: term frequency:词的频率
# idf: inverse document frequency:逆文档频率 == log(总文档数量/该词出现的文档数量)
# tf*idf:重要性程度
```

- 数据预处理使用的模块 **sklearn.preprocessing**



### 类别性数据的处理方式（one-hot编码）

```
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
```



### 数值型数据的处理方式（归一化，标准化）

#### 归一化（一般不使用）

缺点：鲁棒性（稳定性）较差，受异常点的影响

只适合传统精确小数点场景

#### 标准化（常使用）

优点：不容易受异常点的影响，利用方差来求标准差，方差考量数据的稳定性

适合现代嘈杂大数据场景

```python
from sklearn.preprocessing import MinMaxScaler  # 归一化
mms = MinMaxScaler(feature_range=(2, 3))  # 缩放特征值到给定范围
data = mms.fit_transform([[12, 23, 13, 33], [92, 63, 73, 33], [15, 93, 33, 35]])
print(data)
# 1.目的：使得某一个特征不会对最终结果造成更大的影响
# 公式： 
#	X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
# 	X_scaled = X_std * (max - min) + min

from sklearn.preprocessing import StandardScaler  # 标准化
st = StandardScaler()
data = st.fit_transform([[12, 23, 13, 33], [92, 63, 73, 33], [15, 93, 33, 35]])
print(data)

# scale效果同StandardScaler
from sklearn.preprocessing import scale
import numpy as np
def t_scale():
	"""标准化缩放"""
	x_train = np.array([[12, 23, 13, 33], [92, 63, 73, 33], [15, 93, 33, 35]])
	print(x_train)
	data = scale(x_train)
	# data = scale([[12, 23, 13, 33], [92, 63, 73, 33], [15, 93, 33, 35]])
	print(data)
```



### SimpleImputer语法

完成缺失值插补，**一般按列进行插补**

```python
from sklearn.impute import SimpleImputer
import numpy as np
# strategy填补的策略，mean是平均值填补
# verbose填补按行还是列，0按列平均值填补，1按行平均值填补
it = SimpleImputer(missing_values=np.nan, strategy='mean', verbose=0)
data = it.fit_transform([[12, 23, 13, 33], [92, np.nan, 73, 33], [15, 93, 33, 35]])
print(data)
```



### 数据降维

- **特征选择**
- 主要方法
  
  1. filter（过滤式）：VarianceThreshold（方差过滤）
    2. Embedded（嵌入式）：正则化、决策树
    3. Wrapper（包裹式）
    - 目的：
    1.   减少特征数量、降维，使模型泛化能力更强，减少过拟合；
          2. 增强对特征和特征值之间的理解

- **PCA分析**

```
from sklearn.decomposition import PCA
# 本质：PCA是一种分析、简化数据集的技术
# 目的：是数据降维压缩，尽可能降低原数据的维度（复杂度），会损失少量信息
# 作用：可以削减回归分析或者聚类分析中特征的数量
```

​	安装jupyter模板：pip install jupyter

​	在jupyter上编写python代码，启动jupyter：jupyter notebook

​	在jupyter编写代码可以看见表的信息



### 估计器

```python
# 以下是用于分类的估计器
from sklearn.neighbors import *  # k邻近算法
from sklearn.naive_bayes import *  # 贝叶斯算法
from sklearn.linear_model import LogisticRegression  # 逻辑回归
from sklearn.tree import * # 决策树和随机森林
# 以下是用于回归的估计器
from sklearn.linear_model import LogisticRegression  # 线性回归
from sklearn.linear_model import Ridge  # 岭回归
```



### 机器学习开发流程

1. #### 拿到原始数据

   1. 明确问题做什么
   2. 根据数据类型划分应用种类

2. #### 数据的基本处理

   1. pandas去处理数据（缺失值、合并表等等）

3. #### 特征工程

   1. 分类或者回归处理
   2. 制作模型（算法 + 数据）
   3. 找到合适的方法进行预测

4. #### 模型评估

   1. （合格）上线使用，以API形式提供
   2. （不合格）换算法，换参数

### k 近邻算法（实际工程中一般不用）

```python
# k 近邻算法需要做标准化处理
# k 近邻算法公式
# 根号((a1-b1)^2+(a2-b2)^2+(a3-b3)^2)
from sklearn.neighbors import KNeighborsClassifier  # k近邻算法API

# 进行算法流程  knn估计器流程
knn = KNeighborsClassifier(n_neighbors=5)  # k值设为5  ==> n_neighbors=5
knn.fit(x_train, y_train)
# 得出预测结果
y_predict = knn.predict(x_test)
print(f'预测的目标签到位置为：{y_predict}')
# 得出准确率
print(f'预测的准确率：{knn.score(x_test, y_test)}')
```

##### k 取多大的影响

- k 值取小：容易受异常点的影响
- k 值取大：容易受k值数量（类别）波动

##### 优点：

- 简单，易于理解，易于实现，无需估计参数，无需训练

##### 缺点：

- 懒惰算法，对测试样本分类时的计算量大，内存开销大

- 必须指定k值，k值选择不当则分类精度不能保证

 ##### 使用场景：

- 小数据场景，几千 几万样本，具体场景具体业务去测试

```python
API: sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='auto')
        参数：
            n_neighbors: int 可选默认为5（邻居数） ，k_neighbors 查询默认使用的邻居数
            
            algorithm：{'auto', 'ball_tree', 'kd_tree', 'brute'} 可选用与计算最近邻居的算法
            'ball_tree' 将会使用balltrue， kd_tree将使用KDtree ‘auto’ 将尝试更具传递给fit放大的值来决定最合适的算法。（不同实现方法影响效率）
            
        knn = KNeighborsClassifier()
        knn.fit(data) 传入数据
        knn.score(x_test, y_test)  
        knn.predict(x_test)  # 预测特征测试集的目标集
```



### 朴素贝叶斯算法

- 算法不需要调参
- 朴素贝叶斯算法擅长文本分类
- 神经网络对文本分类会比这个方法的效果更好

```python
# 朴素贝叶斯算法公式
# P(科技|影院，支付宝，云计算)=P(影院，支付宝，云计算|科技)*P(科技)
from sklearn.naive_bayes import MultinomialNB  # 朴素贝叶斯算法api

# 进行朴素贝叶斯算法的预测
mlt = MultinomialNB(alpha=1.0)
mlt.fit(x_train, y_train)
y_predict = mlt.predict(x_test)
print(f'预测的准确率为：{mlt.score(x_test, y_test)}')
```

- ##### 朴素贝叶斯算法优点

  - 属性可以离散可以连续
  - 数学基础扎实,分类效率稳定
  - 假设简单,但是在很多实际情况下表现很好
  - 速度较快,一定程度上缓解了维度灾难

- ##### 朴素贝叶斯算法缺点

  - 由于使用样本独立性的假设，所以如果样本属性有关联时，其效果不好
  - 对噪音数据与缺失数据不太敏感
  - 由于训练集当中进行统计词这些工作，会对结果造成干扰















